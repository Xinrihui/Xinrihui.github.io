<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>小灰灰在青青草原</title><meta name="author" content="Xinrihui"><meta name="copyright" content="Xinrihui"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一个计算机民间爱好者的关于算法，分布式系统和机器学习的笔记，欢迎大佬拍砖交流～。笔者曾在微软亚洲研究院实习，参与国家重点研发计划，兴趣为分布式系统和机器学习。转载请注明引用（应该也没人看 &#x3D;&#x3D;），邮箱 xinrihui@outlook.com">
<meta property="og:type" content="website">
<meta property="og:title" content="小灰灰在青青草原">
<meta property="og:url" content="https://xinrihui.github.io/page/2/index.html">
<meta property="og:site_name" content="小灰灰在青青草原">
<meta property="og:description" content="一个计算机民间爱好者的关于算法，分布式系统和机器学习的笔记，欢迎大佬拍砖交流～。笔者曾在微软亚洲研究院实习，参与国家重点研发计划，兴趣为分布式系统和机器学习。转载请注明引用（应该也没人看 &#x3D;&#x3D;），邮箱 xinrihui@outlook.com">
<meta property="og:locale">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:author" content="Xinrihui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xinrihui.github.io/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '小灰灰在青青草原',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2023-03-16 16:47:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="小灰灰在青青草原" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小灰灰在青青草原</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">小灰灰在青青草原</h1></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/2022/12/29/hive%20%E8%BF%9E%E6%8E%A5/" title="hive 连接"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hive 连接"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/29/hive%20%E8%BF%9E%E6%8E%A5/" title="hive 连接">hive 连接</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-29T02:15:20.000Z" title="Created 2022-12-29 10:15:20">2022-12-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/hive-%E7%B3%BB%E5%88%97/">hive 系列</a></span></div><div class="content">


hive 连接1.外连接 和 内连接内连接：两个表都有的才有；最大的表要放在最后面；左连接：左侧表所有的行一定都有半连接：Hive中使用semi join替代exist in加一个子查询取各种集合：F8D754F7-A29F-40FF-B801-8F77995EA7EC.webp2.join 的优化2.1 reduce Join Reduce阶段完成joinSELECTa.id,a.dept,b.ageFROM a join bON (a.id = b.id);纯 mapreduce 实现 joinhttps://www.edureka.co/blog/mapreduce-example-reduce-side-join/2.2 Map JoinMap阶段完成joinMapJoin通常用于一个很小的表和一个大表进行join的场景，具体小表有多小，由参数hive.mapjoin.smalltable.filesize来决定，该参数表示小表的总大小，默认值25M。在0.7版本之后，默认自动会转换Map Join，由参数hive.auto.convert.join来控制，默认为true ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/12/29/hive%20%E5%8E%BB%E9%87%8D%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%20%E5%92%8C%20group%20by%20%E5%8E%9F%E7%90%86/" title="hive 去重的三种方法 和 group by 原理"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hive 去重的三种方法 和 group by 原理"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/29/hive%20%E5%8E%BB%E9%87%8D%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%20%E5%92%8C%20group%20by%20%E5%8E%9F%E7%90%86/" title="hive 去重的三种方法 和 group by 原理">hive 去重的三种方法 和 group by 原理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-29T02:15:20.000Z" title="Created 2022-12-29 10:15:20">2022-12-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/hive-%E7%B3%BB%E5%88%97/">hive 系列</a></span></div><div class="content">


hive 去重的三种方法 和 group by 原理1.去重&nbsp;&nbsp;的三种方法tablename&nbsp;&nbsp;&nbsp;&nbsp;adx&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tran_id&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cost&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; timestampck&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/12/28/2.%20self-attention/" title="Bert的前世今生-1. self-attention"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bert的前世今生-1. self-attention"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/28/2.%20self-attention/" title="Bert的前世今生-1. self-attention">Bert的前世今生-1. self-attention</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-28T06:53:53.000Z" title="Created 2022-12-28 14:53:53">2022-12-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Bert%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Bert的前世今生</a></span></div><div class="content">


2. self-attention1.简单地使用 FC 层来做序列标注，显然没有考虑上下文信息，因此可以在FC层的基础上 增加一个窗口，但是序列的长度会发生变化，窗口的大小不容易设置2.self-attention 中每一个时间步的输出都考虑了所有时间步的输入3.与以前的 soft attention 一样，关键是 计算 注意力的权重 alpha，在 soft attention 中，注意力的权重 alpha 代表的是 解码器的 隐状态 h_t-1 和 编码器的输出向量 a_j 之间的相关程度，在 self-attention 中，注意力的权重 alpha 代表的是 输入向量 a_i 和 输入向量 a_j 的相关程度4.注意力权重 alpha 的计算方法（对齐模型）additive attention利用输出层维度为1 的前馈神经网络 ，得到注意力的权重dot-product attention二者的理论复杂度相同，但是在实践中，dot-product更快，而且节省空间，因为它可以利用很多 优化后的矩阵相乘算子。对于较小的dk而言，这两种机制的性能相似。但是当 dot produ ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/12/28/3.transformer/" title="Bert的前世今生-2.transformer"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bert的前世今生-2.transformer"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/28/3.transformer/" title="Bert的前世今生-2.transformer">Bert的前世今生-2.transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-28T06:53:53.000Z" title="Created 2022-12-28 14:53:53">2022-12-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Bert%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Bert的前世今生</a></span></div><div class="content">


3.transformer1. encoder1.encoder 使用了 self-attention2.一层一层 block&nbsp;&nbsp;的堆叠，单独抽出一个 block&nbsp;&nbsp;来看，简单来说就是 self-attention&nbsp;&nbsp;的输出再加上一个FC&nbsp;&nbsp;就作为这一层 block&nbsp;&nbsp;的输出其实要复杂一些， self-attention&nbsp;&nbsp;的输出&nbsp;&nbsp;在加上&nbsp;&nbsp;残差（residual）连接后&nbsp;&nbsp;输入到 layer normalization&nbsp;&nbsp;再输入到 FC&nbsp;&nbsp;层中&nbsp;&nbsp;3.bert&nbsp;&nbsp;就是 transformer&nbsp;&nbsp;的 encoder&nbsp;&nbsp;的堆叠4.其他的改进（1）residual&nbsp;&nbsp;和 layer normal&nbsp;&nbsp;可以放在别的位置（2）layer normal&n ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/12/28/4.self-supervised%EF%BC%88Bert%20%EF%BC%89/" title="Bert的前世今生-3.self-supervised（Bert）"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bert的前世今生-3.self-supervised（Bert）"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/28/4.self-supervised%EF%BC%88Bert%20%EF%BC%89/" title="Bert的前世今生-3.self-supervised（Bert）">Bert的前世今生-3.self-supervised（Bert）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-28T06:53:53.000Z" title="Created 2022-12-28 14:53:53">2022-12-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Bert%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Bert的前世今生</a></span></div><div class="content">


4.self-supervised（Bert ）1.整整齐齐的一家人2.陷入了军备竞赛3.自监督学习 和&nbsp;&nbsp;传统的监督学习的区别是&nbsp;&nbsp;它的&nbsp;&nbsp;Label&nbsp;&nbsp;是&nbsp;&nbsp;语料 x&nbsp;&nbsp;自己产生的，而不是人工标注的&nbsp;&nbsp;3.自监督&nbsp;&nbsp;的 pre-trainmasked token prediction（做填空题）利用网络上已有的句子，随机地把某个位置的单词盖住，然后让模型去预测被盖住的位置&nbsp;&nbsp;seq2seq显然我们可以得到一个 pre-train&nbsp;&nbsp;的 encoder，&nbsp;&nbsp; 那么如何&nbsp;&nbsp;得到 pre-train&nbsp;&nbsp;的 decoder：输入的 sequence为 w1,w2,w3,w4，训练时的 标签（ground truth） 也为&nbsp;&nbsp;w1,w2,w3,w4，若直接训练，模型是学不到东西的，它会把 w1&nbsp;&n ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/12/28/5.self-supervised%EF%BC%88Bert%20%E5%92%8C%E5%AE%83%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC%EF%BC%89/" title="Bert的前世今生-4.self-supervised（Bert 和它的朋友们）"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bert的前世今生-4.self-supervised（Bert 和它的朋友们）"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/28/5.self-supervised%EF%BC%88Bert%20%E5%92%8C%E5%AE%83%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC%EF%BC%89/" title="Bert的前世今生-4.self-supervised（Bert 和它的朋友们）">Bert的前世今生-4.self-supervised（Bert 和它的朋友们）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-28T06:53:53.000Z" title="Created 2022-12-28 14:53:53">2022-12-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Bert%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">Bert的前世今生</a></span></div><div class="content">


5.self-supervised（Bert 和它的朋友们）Part1. how to pre-train1.在 bert&nbsp;&nbsp;之前&nbsp;&nbsp;我们早就有 pre-train model 了，&nbsp;&nbsp;比如&nbsp;&nbsp;词向量有一个table（矩阵） 去存每一个词（word）的词向量，需要的时候&nbsp;&nbsp;就去里面找对于英文来说，把 token&nbsp;&nbsp;的粒度设置到&nbsp;&nbsp;单词，会有一个问题，就是英文的单词太多了，肯定会有&nbsp;&nbsp;训练语料中没有见过的单词，它肯定不在&nbsp;&nbsp;table&nbsp;&nbsp;里。我们可以把&nbsp;&nbsp;token&nbsp;&nbsp;的粒度设置到&nbsp;&nbsp; 字符（character），把&nbsp;&nbsp;所有的字符输入到模型中，输出该单词的 embedding，这就是 fast-text对于中文，可以把&nbsp;&nbsp;文字图片丢给 CNN，让模型去学&nbsp;&nbsp;中文的形态 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/12/28/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/" title="Transformer 论文和源码分析"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 论文和源码分析"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/28/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/" title="Transformer 论文和源码分析">Transformer 论文和源码分析</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-28T06:47:10.000Z" title="Created 2022-12-28 14:47:10">2022-12-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/transformer-%E7%B3%BB%E5%88%97/">transformer 系列</a></span></div><div class="content">


Transformer 论文和源码分析Part1. 论文1.Transformer 完全依赖于注意力机制来 表达 输入和输出之间的 全局依赖关系，而没有采用 循环结构 2.Transformer  采用了 编码器-解码器 结构，  （1）编码器将符号表示形式（x1; :::; xn）的输入序列映射到 连续表示形式 z =（z1; :::; zn）的序列。  （2）上一步拿到z后，解码器一次生成一个符号的符号的输出序列（y1; :::; ym）。 模型的每一步都是自回归的，在生成下一步的输出时，会将 上一步 生成的符号用作附加输入。3.Multi-Head Attentionmulti-head attention 是由 h 个 scaled dot-product attention 组成的与 d_model个 维度 一起输入 注意力函数 （ single-head attention ）相比，我们发现 把 总维度 线性投影 h次 有更好的效果。对每一个子空间，我们可以并行的 计算他们的注意力函数，最后得到 dv 维度的 输出value。我们最后会把这些 子空间 连接起来 ，并再 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/12/25/1.shuffle%20%E7%A0%94%E7%A9%B6/" title="Spark shuffle 机制"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark shuffle 机制"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/25/1.shuffle%20%E7%A0%94%E7%A9%B6/" title="Spark shuffle 机制">Spark shuffle 机制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-25T01:54:24.000Z" title="Created 2022-12-25 09:54:24">2022-12-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/spark-%E7%B3%BB%E5%88%97/">spark 系列</a></span></div><div class="content">


1.shuffle 研究总结1.管道执行 pipeline操作都在内存中执行，不需要为了把计算结果给其他任务而将数据落盘2.Shuffle是 map reduce 的中间过程。map 和 reduce 过程经常被提及 而shuffle不配有姓名，但它恰恰是 开销最大 也最值得优化的步骤3.排序mapreduce 是所有的 shuffle 都排序，排序的目的是： 在O(1) 的空间复杂度下在 reduce 端实现 key  的聚合；spark 在一开始的版本中，为了节约排序的时间代价（快排 O(nlogn)），在 reduce 端 采用 内存中的 hashmap  来做 key  的聚合，当内存写满后再 溢写到磁盘中4.Pipline &amp; writing diskmapreduce 是全部落盘，spark 是遇到shuffle 才落盘， MPP是全部pipeline；Pipeline越多 延迟越低 容错越差（没有检查点 错了就回到原点 重头再来） 任务调度更加精确 带来了系统复杂度上升（我这边计算好了要交给你 你注意接收）那么系统的吞吐量和可扩展性必然下降5.宽依赖 和 窄 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/12/25/2.RDD%20%E7%AE%97%E5%AD%90/" title="Spark RDD 算子"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark RDD 算子"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/25/2.RDD%20%E7%AE%97%E5%AD%90/" title="Spark RDD 算子">Spark RDD 算子</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-25T01:54:24.000Z" title="Created 2022-12-25 09:54:24">2022-12-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/spark-%E7%B3%BB%E5%88%97/">spark 系列</a></span></div><div class="content">


2.RDD 算子读写文件spark默认读取 HDFS中的文件（hdfs://Master:9000/tmp/hive1）sc.textFile("hdfs://Master:9000/tmp/people") // idea IDE默认在file:///里找。最好指明文件路径 是在 HDFS 还是在本地 
读取本地文件:val rdd=sc.textFile("file:////app/hadoop/spark110/NOTICE")   //必须在所有datanode中都有，而且都为此路径windows 下的 本地文件 路径必须使用 '\\' 隔开data_dir = 'data\\' #   当前文件夹下有 data目录

tmp_index_file_dir = data_dir + 'tmp_index_spark.bin'  
lines = sc.textFile(tmp_index_file_dir,8)问题： 如何控制 输出的文件 只有一个切片lines = sc.textFile(tmp_index_file_dir,8) 
# 对大文件 进行切片 sliceN ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/12/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%20-%E6%95%B0%E6%8D%AEsense/" title="计算机组成原理 -数据sense"><img class="post_bg" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机组成原理 -数据sense"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/12/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%20-%E6%95%B0%E6%8D%AEsense/" title="计算机组成原理 -数据sense">计算机组成原理 -数据sense</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-24T16:17:10.000Z" title="Created 2022-12-25 00:17:10">2022-12-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/">计算机组成原理</a></span></div><div class="content">


计算机组成原理 -数据sense1.不同硬件的性能指标LatencyLatencyNotesNotes2L1 cache reference  （读取CPU的一级缓存）0.5 nsBranch mispredict(转移、分支预测)5   nsL2 cache reference（ 读取CPU的二级缓存）7   ns14x L1 cacheL3 cache reference  20   nsMutex lock/unlock（互斥锁\解锁）25   nsMain memory reference（读取内存数据）100   ns  200x L1 cache20x L2 cache, Compress 1K bytes with Zippy10,000   ns10 usSend 1 KB bytes over 1 Gbps network10,000   ns10 usRead 4 KB randomly from SSD*150,000   ns150 usRead 1 MB sequentially from memory250,000   ns250 usRound tr ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xinrihui</div><div class="author-info__description">一个计算机民间爱好者的关于算法，分布式系统和机器学习的笔记，欢迎大佬拍砖交流～。笔者曾在微软亚洲研究院实习，参与国家重点研发计划，兴趣为分布式系统和机器学习。转载请注明引用（应该也没人看 ==），邮箱 xinrihui@outlook.com</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xinrihui"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/03/16/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/" title="文本相似度"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文本相似度"/></a><div class="content"><a class="title" href="/2023/03/16/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/" title="文本相似度">文本相似度</a><time datetime="2023-03-16T08:46:48.000Z" title="Created 2023-03-16 16:46:48">2023-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/16/Evaluation/" title="Evaluation"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Evaluation"/></a><div class="content"><a class="title" href="/2023/03/16/Evaluation/" title="Evaluation">Evaluation</a><time datetime="2023-03-16T08:15:25.000Z" title="Created 2023-03-16 16:15:25">2023-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/16/Learning%20to%20Rank/" title="Learning to Rank"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Learning to Rank"/></a><div class="content"><a class="title" href="/2023/03/16/Learning%20to%20Rank/" title="Learning to Rank">Learning to Rank</a><time datetime="2023-03-16T08:15:25.000Z" title="Created 2023-03-16 16:15:25">2023-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E7%9A%84%20lambda%20%E6%A1%86%E6%9E%B6/" title="大数据系统的 lambda 框架"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大数据系统的 lambda 框架"/></a><div class="content"><a class="title" href="/2023/03/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E7%9A%84%20lambda%20%E6%A1%86%E6%9E%B6/" title="大数据系统的 lambda 框架">大数据系统的 lambda 框架</a><time datetime="2023-03-16T01:22:30.000Z" title="Created 2023-03-16 09:22:30">2023-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/" title="分布式系统综述"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="分布式系统综述"/></a><div class="content"><a class="title" href="/2023/03/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/" title="分布式系统综述">分布式系统综述</a><time datetime="2023-03-10T06:59:05.000Z" title="Created 2023-03-10 14:59:05">2023-03-10</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Bert%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"><span class="card-category-list-name">Bert的前世今生</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Web/"><span class="card-category-list-name">Web</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/XGBoost-%E7%B3%BB%E5%88%97/"><span class="card-category-list-name">XGBoost 系列</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/hive-%E7%B3%BB%E5%88%97/"><span class="card-category-list-name">hive 系列</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/spark-%E7%B3%BB%E5%88%97/"><span class="card-category-list-name">spark 系列</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/transformer-%E7%B3%BB%E5%88%97/"><span class="card-category-list-name">transformer 系列</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"><span class="card-category-list-name">从零开始搭建搜索引擎</span><span class="card-category-list-count">7</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA/"><span class="card-category-list-name">信息检索导论</span><span class="card-category-list-count">3</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/AdaBoost/" style="font-size: 1.1em; color: #999">AdaBoost</a> <a href="/tags/B-%E6%A0%91/" style="font-size: 1.1em; color: #999">B+树</a> <a href="/tags/Bleu/" style="font-size: 1.1em; color: #999">Bleu</a> <a href="/tags/CNN/" style="font-size: 1.1em; color: #999">CNN</a> <a href="/tags/GBDT/" style="font-size: 1.1em; color: #999">GBDT</a> <a href="/tags/HBase/" style="font-size: 1.3em; color: #99a1ac">HBase</a> <a href="/tags/IOPS/" style="font-size: 1.1em; color: #999">IOPS</a> <a href="/tags/LDA/" style="font-size: 1.1em; color: #999">LDA</a> <a href="/tags/LSTM/" style="font-size: 1.1em; color: #999">LSTM</a> <a href="/tags/LambdaRank/" style="font-size: 1.1em; color: #999">LambdaRank</a> <a href="/tags/Lambdamart/" style="font-size: 1.1em; color: #999">Lambdamart</a> <a href="/tags/MAP/" style="font-size: 1.1em; color: #999">MAP</a> <a href="/tags/MLP/" style="font-size: 1.1em; color: #999">MLP</a> <a href="/tags/MPP/" style="font-size: 1.1em; color: #999">MPP</a> <a href="/tags/MapReduce/" style="font-size: 1.1em; color: #999">MapReduce</a> <a href="/tags/NDCG/" style="font-size: 1.1em; color: #999">NDCG</a> <a href="/tags/OLAP/" style="font-size: 1.1em; color: #999">OLAP</a> <a href="/tags/PCA/" style="font-size: 1.1em; color: #999">PCA</a> <a href="/tags/Phoenix/" style="font-size: 1.1em; color: #999">Phoenix</a> <a href="/tags/RNN/" style="font-size: 1.1em; color: #999">RNN</a> <a href="/tags/RankNet/" style="font-size: 1.1em; color: #999">RankNet</a> <a href="/tags/bert/" style="font-size: 1.2em; color: #999da3">bert</a> <a href="/tags/bm25/" style="font-size: 1.5em; color: #99a9bf">bm25</a> <a href="/tags/hash%E8%A1%A8/" style="font-size: 1.1em; color: #999">hash表</a> <a href="/tags/hexo/" style="font-size: 1.1em; color: #999">hexo</a> <a href="/tags/hive/" style="font-size: 1.3em; color: #99a1ac">hive</a> <a href="/tags/java/" style="font-size: 1.1em; color: #999">java</a> <a href="/tags/lambda-%E6%A1%86%E6%9E%B6/" style="font-size: 1.1em; color: #999">lambda 框架</a> <a href="/tags/learning-to-rank/" style="font-size: 1.1em; color: #999">learning to rank</a> <a href="/tags/listwise/" style="font-size: 1.1em; color: #999">listwise</a> <a href="/tags/nlp/" style="font-size: 1.4em; color: #99a5b6">nlp</a> <a href="/tags/node-js/" style="font-size: 1.1em; color: #999">node.js</a> <a href="/tags/pairwise/" style="font-size: 1.1em; color: #999">pairwise</a> <a href="/tags/paxo/" style="font-size: 1.1em; color: #999">paxo</a> <a href="/tags/spark/" style="font-size: 1.3em; color: #99a1ac">spark</a> <a href="/tags/tf-idf/" style="font-size: 1.5em; color: #99a9bf">tf-idf</a> <a href="/tags/transformer/" style="font-size: 1.4em; color: #99a5b6">transformer</a> <a href="/tags/word2vec/" style="font-size: 1.1em; color: #999">word2vec</a> <a href="/tags/xgboost/" style="font-size: 1.4em; color: #99a5b6">xgboost</a> <a href="/tags/%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/" style="font-size: 1.1em; color: #999">三阶段提交</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/03/"><span class="card-archive-list-date">March 2023</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/12/"><span class="card-archive-list-date">December 2022</span><span class="card-archive-list-count">34</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">40</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2023-03-16T08:47:59.439Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Xinrihui</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
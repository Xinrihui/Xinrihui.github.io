<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spark shuffle 机制 | 小灰灰在青青草原</title><meta name="author" content="Xinrihui"><meta name="copyright" content="Xinrihui"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1.shuffle 研究总结1.管道执行 pipeline操作都在内存中执行，不需要为了把计算结果给其他任务而将数据落盘2.Shuffle是 map reduce 的中间过程。map 和 reduce 过程经常被提及 而shuffle不配有姓名，但它恰恰是 开销最大 也最值得优化的步骤3.排序mapreduce 是所有的 shuffle 都排序，排序的目的是： 在O(1) 的空间复杂度下在">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark shuffle 机制">
<meta property="og:url" content="https://xinrihui.github.io/2022/12/25/1.shuffle%20%E7%A0%94%E7%A9%B6/index.html">
<meta property="og:site_name" content="小灰灰在青青草原">
<meta property="og:description" content="1.shuffle 研究总结1.管道执行 pipeline操作都在内存中执行，不需要为了把计算结果给其他任务而将数据落盘2.Shuffle是 map reduce 的中间过程。map 和 reduce 过程经常被提及 而shuffle不配有姓名，但它恰恰是 开销最大 也最值得优化的步骤3.排序mapreduce 是所有的 shuffle 都排序，排序的目的是： 在O(1) 的空间复杂度下在">
<meta property="og:locale">
<meta property="article:published_time" content="2022-12-25T01:54:24.000Z">
<meta property="article:modified_time" content="2022-12-29T02:04:23.575Z">
<meta property="article:author" content="Xinrihui">
<meta property="article:tag" content="spark">
<meta property="article:tag" content="分布式计算框架">
<meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xinrihui.github.io/2022/12/25/1.shuffle%20%E7%A0%94%E7%A9%B6/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark shuffle 机制',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-29 10:04:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="小灰灰在青青草原" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">34</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">40</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小灰灰在青青草原</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark shuffle 机制</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-12-25T01:54:24.000Z" title="Created 2022-12-25 09:54:24">2022-12-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-12-29T02:04:23.575Z" title="Updated 2022-12-29 10:04:23">2022-12-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/spark-%E7%B3%BB%E5%88%97/">spark 系列</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark shuffle 机制"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container">
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 9.6.1 (469462)"/><meta name="author" content="羊村的好朋友小灰灰"/><meta name="created" content="2022-12-07 09:55:36 +0000"/><meta name="source" content="yinxiang.superNote"/><meta name="source-application" content="yinxiang.win32"/><meta name="source-url" content="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html"/><meta name="updated" content="2022-12-24 15:29:44 +0000"/><title>1.shuffle 研究</title></head><body><h1>总结</h1><div><br/></div><div><span style="font-size: 12pt;">1.管道执行 pipeline</span></div><div><span style="font-size: 12pt;">操作都在内存中执行，不需要为了把计算结果给其他任务而将数据落盘</span></div><div><br/></div><div><br/></div><div><span style="font-size: 12pt;">2.Shuffle是 map reduce 的中间过程。map 和 reduce 过程经常被提及 而shuffle不配有姓名，但它恰恰是 开销最大 也最值得优化的步骤</span></div><div><br/></div><div><span style="font-size: 12pt;">3.排序</span></div><div><br/></div><div><span style="font-size: 12pt;">mapreduce 是所有的 shuffle 都排序，排序的目的是： 在O(1) 的空间复杂度下在 reduce 端实现 key  的聚合；</span></div><div><br/></div><div><span style="font-size: 12pt;">spark 在一开始的版本中，为了节约排序的时间代价（快排 O(nlogn)），在 reduce 端 采用 内存中的 hashmap  来做 key  的聚合，当内存写满后再 溢写到磁盘中</span></div><div><br/></div><div><span style="font-size: 12pt;">4.Pipline &amp; writing disk</span></div><div><span style="font-size: 12pt;">mapreduce 是全部落盘，spark 是遇到shuffle 才落盘， MPP是全部pipeline；</span></div><div><span style="font-size: 12pt;">Pipeline越多 延迟越低 容错越差（没有检查点 错了就回到原点 重头再来） 任务调度更加精确 带来了系统复杂度上升（我这边计算好了要交给你 你注意接收）那么系统的吞吐量和可扩展性必然下降</span></div><div><br/></div><div><span style="font-size: 12pt;">5.宽依赖 和 窄依赖</span></div><div><br/></div><div><span style="font-size: 12pt;">窄依赖：一个子分区只依赖于一个 父分区，或者依赖于多个完整的父分区 （因为完整所以不需要把分区打散了传送）。</span></div><div><br/></div><div><span style="font-size: 12pt;">宽依赖：一个子分区 依赖多个父分区的其中一部分（因为不完整，需要将分区打散了再发送）。</span></div><div><br/></div><hr/><div><br/></div><h1>尽量避免使用shuffle类算子</h1><div><br/></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">shuffle是一个涉及到CPU(序列化反序列化)、网络IO(跨节点数据传输)以及磁盘IO(shuffle中间结果落地)的操作，所以它是最消耗性能的操作</span>。</span></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">shuffle过程就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作</span>。比如reduceByKey、join等算子，都会触发shuffle操作。</span></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点（优先放入内存中）进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中</span>。</span></div><div><br/></div><div><span style="font-size: 12pt;">因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</span></div><div><br/></div><h2>方案1.map 端预聚合</h2><div><span style="font-size: 12pt;">在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，<span style="font-weight: bold;">建议使用reduceByKey或者aggregateByKey算子</span>来<span style="font-weight: bold;">替代掉groupByKey算子</span>。<span style="font-weight: bold;">因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合</span>。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</span></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;"> groupByKey                                                                                                     </span></span></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/8307D049-2768-4F48-B093-A2D2B865F7AB.png"/><div><span style="font-size: 12pt;"><span style="font-weight: bold;"> reduceByKey</span></span></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/28927950-4E28-4D39-911C-27EFFFF657B1.png"/><div><br/></div><div><br/></div><h2>方案２.广播</h2><div><br/></div><div><span style="font-size: 12pt;">使用广播，把较小的表广播出去，相当于每个节点都复制了一份</span></div><div><span style="font-size: 12pt;">// 传统的join操作会导致shuffle操作。</span></div><div><span style="font-size: 12pt;">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></div><div><span style="font-size: 12pt;">val rdd3 = rdd1.join(rdd2)</span></div><div><br/></div><div><span style="font-size: 12pt;">// Broadcast+map的join操作，不会导致shuffle操作。</span></div><div><span style="font-size: 12pt;">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></div><div><span style="font-size: 12pt;">val rdd2Data = rdd2.collect()</span></div><div><span style="font-size: 12pt;">val rdd2DataBroadcast = sc.broadcast(rdd2Data)</span></div><div><br/></div><div><span style="font-size: 12pt;">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></div><div><span style="font-size: 12pt;">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></div><div><span style="font-size: 12pt;">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></div><div><span style="font-size: 12pt;">val rdd3 = rdd1.map(rdd2DataBroadcast...)</span></div><div><br/></div><div><span style="font-size: 12pt;">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></div><div><span style="font-size: 12pt;">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></div><div><br/></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">引用：</span></span></div><div><a target="_blank" rel="noopener" href="https://tech.meituan.com/spark_tuning_basic.html" rev="en_rl_none"><span style="font-size: 12pt;"><u>https://tech.meituan.com/spark_tuning_basic.html</u></span></a></div><div><br/></div><div><br/></div><h1>数据倾斜问题</h1><div><br/></div><p style="--en-paragraph:true;"><span style="font-size: 12pt;"><span style="font-weight: bold;">现象：</span></span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">1.绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时</span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">2.原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。</span></p><div><br/></div><p style="--en-paragraph:true;"><span style="font-size: 12pt;"><span style="font-weight: bold;">原理：</span></span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">1.在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。举个例子：</span></p><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/D5F2C86A-0C55-4805-9390-0450E87ECE84.png"/><p style="--en-paragraph:true;"><br/></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">2.可能会触发shuffle操作的RDD算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。</span></p><p style="--en-paragraph:true;"><br/></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;"><span style="font-weight: bold;">解决方案</span></span></p><p style="--en-paragraph:true;"><br/></p><h2>1.聚合（groupByKey）</h2><p style="--en-paragraph:true;"><br/></p><div><span style="font-size: 12pt;">对于聚合类的shuffle ，可以采用两阶段聚合（局部聚合+全局聚合）</span></div><div><br/></div><p style="--en-paragraph:true;"><span style="font-size: 12pt;">第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)</span></p><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/3010811A-0551-46B8-A2E8-9B78C7610D2C.png"/><h2>２.连接(join)</h2><p style="--en-paragraph:true;"><br/></p><h3>1.reduce join 转换为 map join</h3><p style="--en-paragraph:true;"><br/></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（1）reduce join：走shuffle过程，一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join。</span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（2）map join：将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后<span style="font-weight: bold;">对其创建一个Broadcast变量</span>；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（3）缺点：我们需要将小表进行广播，此时会比较消耗内存资源，<span style="font-weight: bold;">driver和每个Executor内存中都会驻留一份小RDD的全量数据</span>。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。</span></p><p style="--en-paragraph:true;"><br/></p><h3>2.采样倾斜的key并分拆join</h3><div><br/></div><p style="--en-paragraph:true;"><span style="font-size: 12pt;"><b><span style="font-weight: 700;">方案实现思路：</span></b> </span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（1）对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 </span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（2）然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 </span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;"> （3）接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 </span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（4）再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 </span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（5）而另外两个普通的RDD就照常join即可。 </span></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">（6）最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</span></p><p style="--en-paragraph:true;"><br/></p><p style="text-align:start;"><span style="font-size: 12pt;"><b><span style="font-weight: 700;">方案实现原理：</span></b>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了</span></p><p style="text-align:start;"><br/></p><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/4D110DDA-A219-4A32-9A6C-09F8B67C6702.png"/><p style="--en-paragraph:true;"><br/></p><p style="--en-paragraph:true;"><br/></p><div><br/></div><p style="--en-paragraph:true;"><span style="font-size: 12pt;"><span style="font-weight: bold;">总结：</span>针对原始RDD进行join操作时候遇到的种种问题，spark提供了高层抽象spark SQL，它将完成上述的SQL优化</span></p><p style="--en-paragraph:true;"><br/></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;"><span style="font-weight: bold;">引用：</span></span></p><p style="--en-paragraph:true;"><a target="_blank" rel="noopener" href="https://tech.meituan.com/spark_tuning_pro.html" rev="en_rl_none"><span style="font-size: 12pt;">https://tech.meituan.com/spark_tuning_pro.html</span></a></p><p style="--en-paragraph:true;"><br/></p><h1>spark SQL 实现 join 的物理计划</h1><div><br/></div><h2>1.Broadcast (hash) Join</h2><div><span style="font-size: 12pt;">在SparkSQL中，对两个表做Join最直接的方式是先根据key分区，再在每个分区中把key值相同的记录拿出来做连接操作。但这样就不可避免地涉及到shuffle，而shuffle在Spark中是比较耗时的操作，我们应该尽可能的设计Spark应用使其避免大量的shuffle。步骤如下：</span></div><div><span style="font-size: 12pt;">1. broadcast阶段：将小表广播分发到大表所在的所有主机。广播算法可以有很多，最简单的是先发给driver，driver再统一分发给所有executor；要不就是基于bittorrete的p2p思路；</span></div><div><span style="font-size: 12pt;">2. hash join阶段：在每个executor上执行单机版hash join，小表映射，大表试探</span></div><div><span style="font-size: 12pt;">executor存储小表的全部数据，一定程度上牺牲了空间，换取shuffle操作大量的耗时（类似于 hive 的mapjoin ）</span></div><div><br/></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/695F22E9-6704-42E1-9873-07EDAE923B5D.png" height="456" width="831"/><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">缺点：</span>这个方案只能用于广播较小的表，否则数据的冗余传输就远大于shuffle的开销；另外，广播时需要将被广播的表现collect到driver端，当频繁有广播出现时，对driver的内存也是一个考验。</span></div><div><span style="font-size: 12pt;">只有当表的大小在 spark.sql.autoBroadcastJoinThreshold 的设置值（默认为 10MB）之内，才会启用 broadcast join，否则采用sort merge join。此方法适合 一张大表和一张非常小的小表 join。</span></div><div><br/></div><h2>2.shuffle Hash Join （ 重分区 hash join ）</h2><div><br/></div><div><span style="font-size: 12pt;">如果一张表很小，执行join操作最优的选择无疑是broadcast hash join，效率最高。但是一旦小表数据量增大，广播所需内存、带宽等资源必然就会太大，broadcast hash join就不再是最优方案。此时可以按照join key进行分区，根据key相同必然分区相同的原理，就可以将大表join分而治之，划分为很多小表的join，充分利用集群资源并行化。</span></div><div><span style="font-size: 12pt;">分为两步：</span></div><div><span style="font-size: 12pt;">1. 对两张表分别按照 join keys进行重分区，即shuffle，目的是为了让 有相同join keys值的记录分到对应的分区中</span></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/9D78FE48-5048-47F8-A919-45E85E0A7A19.png" height="293" width="876"/><div><span style="font-size: 12pt;">2. 对 各个 分区中的数据 进行join：先将 <span style="font-weight: bold;">小表的分区构造为一张hash表</span>，然后根据大表分区中记录的join keys值拿出来进行匹配；</span></div><div><br/></div><div><span style="font-size: 12pt;">优点：两张表 对应的 两个分区 自己做 join ，不用 和 其他的分区 做关联，降低开销</span></div><div><br/></div><div><span style="font-size: 12pt;">Shuffle Hash Join的条件有以下几个：</span></div><div><br/></div><div><span style="font-size: 12pt;">1）分区的平均大小不超过spark.sql.autoBroadcastJoinThreshold所配置的值，默认是10M</span></div><div><span style="font-size: 12pt;">2） 开启 尝试使用hash join的开关，spark.sql.join.preferSortMergeJoin=false</span></div><div><span style="font-size: 12pt;">3）一侧的表要明显小于另外一侧，小的一侧将被hash（明显小于的定义为3倍小，此处为经验值）</span></div><div><br/></div><div><span style="font-size: 12pt;">我们可以看到，在一定大小的表中，SparkSQL从时空结合的角度来看，将两个表进行重新分区，并且对小表中的分区进行hash化，从而完成join。在保持一定复杂度的基础上，尽量减少driver和executor的内存压力，提升了计算时的稳定性。</span></div><div><br/></div><div><br/></div><h2>3.shuffle Sort Merge Join （ 重分区 排序 join ）</h2><div><br/></div><div><span style="font-size: 12pt;">首先将两张表按照join keys进行了重新shuffle，保证join keys值相同的记录会被分在相应的分区。</span></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/7675B305-2BE3-4D1D-9C37-2CE5B9423E5F.png" height="443" width="859"/><div><br/></div><div><span style="font-size: 12pt;">分区后对每个分区内的数据进行排序（<span style="font-weight: bold;">spark shuffle 阶段 自动排序</span>），排序后再对相应的分区内的记录进行连接。因为两个序列都是有序的，从头遍历，碰到key相同的就输出；如果不同，左边小就继续取左边，反之取右边。可以看出，无论分区有多大，Sort Merge Join都不用把某一侧的数据全部加载到内存中，而是 即用 即取 即丢，从而大大提升了大数据量下sql join的稳定性。</span></div><div><br/></div><div><span style="font-size: 12pt;">这是spark SQL 默认的join 方法，适合两张表都是大表的情况。</span></div><div><br/></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/2AD3BD2D-330D-4E33-B6F6-B346A2392C93.png" height="615" width="1079"/><div><br/></div><div><span style="font-size: 12pt;">其实 <span style="font-weight: bold;">hash join</span> 的思路来源于传统的数据库</span></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">Hash Join</span></span></div><div><span style="font-size: 12pt;">先来看看这样一条SQL语句：select * from order,item where item.id = order.i_id，很简单一个Join节点，参与join的两张表是item和order，join key分别是item.id以及order.i_id。现在假设这个Join采用的是hash join算法，整个过程会经历三步：</span></div><div><span style="font-size: 12pt;">1） 确定Build Table以及Probe Table：这个概念比较重要，Build Table使用join key构建Hash Table，而Probe Table使用join key进行探测，探测成功就可以join在一起。通常情况下，小表会作为Build Table，大表作为Probe Table。此事例中item为Build Table，order为Probe Table。</span></div><div><span style="font-size: 12pt;">2） 构建Hash Table：依次读取Build Table（item）的数据，对于每一行数据根据join key（item.id）进行hash，hash到对应的Bucket，生成hash table中的一条记录。数据缓存在内存中，如果内存放不下需要dump到外存。</span></div><div><span style="font-size: 12pt;">3） 探测：再依次扫描Probe Table（order）的数据，使用相同的hash函数映射Hash Table中的记录，映射成功之后再检查join条件（item.id = order.i_id），如果匹配成功就可以将两者join在一起。</span></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/54886F94-64CA-45E3-B3A6-F85E54FE56F3.png" height="493" width="805"/><div><span style="font-size: 12pt;">很显然，hash join基本都只扫描两表一次，可以认为o(a+b)，较之最极端的笛卡尔集运算a*b，时间复杂度低。</span></div><p style="--en-paragraph:true;"><br/></p><p style="--en-paragraph:true;"><span style="font-size: 12pt;">Join操作是传统数据库中的一个高级特性，尤其对于当前MySQL数据库更是如此，原因很简单，MySQL对Join的支持目前还比较有限，只支持Nested-Loop Join算法，因此在OLAP场景下MySQL是很难吃的消的，不要去用MySQL去跑任何OLAP业务，结果真的很难看。和MySQL相比，PostgreSQL、SQLServer、Oracle等这些数据库对Join支持更加全面一些，都支持Hash Join算法。总体而言，传统数据库单机模式做Join的场景毕竟有限，也建议尽量减少使用Join。然而大数据领域就完全不同，Join是标配，OLAP业务根本无法离开表与表之间的关联，对Join的支持成熟度一定程度上决定了系统的性能，夸张点说，’得Join者得天下’。</span></p><p style="--en-paragraph:true;"><br/></p><div><span style="font-size: 12pt;"><span style="font-weight: bold;">引用：</span></span></div><div><a target="_blank" rel="noopener" href="http://hbasefly.com/2017/03/19/sparksql-basic-join/" rev="en_rl_none"><span style="font-size: 12pt;">http://hbasefly.com/2017/03/19/sparksql-basic-join/</span></a></div><div><a target="_blank" rel="noopener" href="http://sharkdtu.com/posts/spark-sql-join.html" rev="en_rl_none"><span style="font-size: 12pt;">http://sharkdtu.com/posts/spark-sql-join.html</span></a></div><div><br/></div><h1><span style="color: #000000;">有的 join 其实属于 窄依赖 </span></h1><div><br/></div><ul><li><div><span style="font-size: 12pt;"><span style="font-weight: bold;">窄依赖（不发生 shuffle）</span>：父Rdd的分区最多只能被一个子Rdd的分区所引用，即一个父Rdd的分区对应一个子Rdd的分区，或者多个父Rdd的分区对应一个子Rdd的分区。即<span style="font-weight: bold;">一对一 或 多对一（儿子有多个父亲）</span>，如下图左边所示。</span></div></li></ul><div><br/></div><ul><li><div><span style="font-size: 12pt;"><span style="font-weight: bold;">宽依赖（发生shuffle）</span>：RDD的分区依赖于父RDD的多个分区或所有分区，即存在一个父RDD的一个分区对应一个子RDD的多个分区。1个父RDD分区对应多个子RDD分区，这其中又分两种情况：1个父RDD对应所有子RDD分区（未经协同划分的Join）或者1个父RDD对应非全部的多个RDD分区（如groupByKey），即<span style="font-weight: bold;">一对多（父亲有多个儿子）。</span></span></div></li></ul><div><br/></div><div><span style="font-size: 12pt;">（1）图中左半部分join：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)；</span></div><div><br/></div><div><span style="font-size: 12pt;">（2）图中右半部分join：其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。</span></div><div><br/></div><div><br/></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/Image.png" height="410" width="565"/><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">join with inputs co-partitions</span>：</span></div><div><span style="font-size: 12pt;">如果 两张表的 分区 是一一对应的， 相当于 <span style="font-weight: bold;">分区hash join</span> 的 第一步已经 天然做好了，因此 只要 两张表的 对应两个分区（相当于 两个子表 ）自己 做Join 即可，</span></div><div><span style="font-size: 12pt;">具体可以采用 hash Join ，若分区 内已经有序，还可以使用 sort merge Join 。</span></div><div><span style="font-size: 12pt;">在这种情况下，子RDD 的一个分区的 数据 不会 “可能来自于 所有的 父RDD” ，所以是窄依赖。</span></div><div><br/></div><h1>shuffle管理器</h1><div><br/></div><div><span style="font-size: 12pt;">在Spark 1.2以前，默认的 shuffle计算引擎 是HashShuffleManager。它有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。因此在Spark 1.2以后的版本中，默认的 ShuffleManager 改成了 SortShuffleManager</span></div><div><br/></div><h2>HashShuffleManager</h2><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">shuffle write</span>：主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是<span style="font-weight: bold;">对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task</span>。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。如图所示，下一个stage 一共有3个task ，当前的每一个task都要创建3个文件。可见，未经优化的<span style="font-weight: bold;">shuffle write操作所产生的磁盘文件的数量是极其惊人的。（MapReduce 也存在此问题）</span></span></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">shuffle read</span>：此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</span></div><div><br/></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/a8d32434d35045b4b05ef6bcc7ce4fba.jpeg" height="564" width="957"/><div><br/></div><div><span style="font-size: 12pt;">优化后的Hash Shuffle</span></div><div><span style="font-size: 12pt;">普通机制Hash Shuffle会产生大量的小文件(M * R），对文件系统的压力也很大，也不利于IO的吞吐量，后来做了优化（设置spark.shuffle.consolidateFiles=true开启，默认false），把在同一个core上的多个Mapper输出到同一个文件，这样文件数就变成core * R 个了。如下图所示：2个core 4个map task 3 个reduce task，会产生2*3=6个小文件。</span></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/24e46dede4474c3d8d9349bb93579a4b.jpeg" height="736" width="957"/><div><br/></div><div><span style="font-size: 12pt;">Hash shuffle合并机制的问题：如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。进而引出了更优化的sort shuffle。</span></div><div><br/></div><h2>SortShuffleManager</h2><div><br/></div><h3>普通运行机制</h3><div><br/></div><div><span style="font-size: 12pt;">在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。</span></div><ul><li><div><span style="font-size: 12pt;">如果是 reduceByKey 这种 <span style="font-weight: bold;">聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存</span>；</span></div></li><li><div><span style="font-size: 12pt;">如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。</span></div></li></ul><div><span style="font-size: 12pt;">接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。<span style="font-weight: bold;">在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序</span>。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。</span></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">最后会将之前所有的临时磁盘文件都进行合并</span>，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，<span style="font-weight: bold;">由于一个当前task就只对应一个磁盘文件</span>，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份<span style="font-weight: bold;">索引文件，其中标识了下游各个task的数据 在文件中的start offset与end offset</span>。</span></div><div><br/></div><div><span style="font-size: 12pt;">下游的 task 的数量 取决于 分区函数( eg. partitionBy , repartition ) 所设定的分区数目 ，这里的下游 相当于 MapReduce 中的 reduce 阶段 ，在reduce 阶段 ：一个分区对应一个 reduce task ，它自己去拉取 各个Map task 生成的属于自己的分区文件 ； </span></div><div><span style="font-size: 12pt;">这里的优化策略可以概括为，对于 一个 Map task，它本来要生成 多个分区文件，我们将它合并到一个文件中，并搭配一个索引文件，这样大大减少了文件数量。</span></div><div><br/></div><div><br/></div><div><br/></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/C7C9849D-7F60-4F02-8F46-806CBB693D89.png"/><div><br/></div><h3>bypass 运行机制</h3><div><br/></div><img src="/Resources/1.shuffle%20%E7%A0%94%E7%A9%B6.resources/e46b3cd956dd4f1493fa6218d842be2d.jpeg" height="694" width="1080"/><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 12pt;">触发条件：</span></div><ul><li><div><span style="font-size: 12pt;">shuffle map task 数量小于 spark.shuffle.sort.bypassMergeThreshold 参数设定值。</span></div></li></ul><div><span style="font-size: 12pt;">AND</span></div><ul><li><div><span style="font-size: 12pt;">join 类算子（由于在普通的运行机制下，对于聚合类算子，会预先进行聚合再写入文件，性能更优）</span></div></li></ul><div><br/></div><div><span style="font-size: 12pt;">此时task会为每个reduce端的task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</span></div><div><br/></div><div><span style="font-size: 12pt;">该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</span></div><div><br/></div><div><br/></div><h1>为什么 shuffle 的中间结果落磁盘 而不落在内存</h1><div><br/></div><p>在目前的 Spark 实现中，shuffle block 一定是落地到磁盘的，无法像普通 RDD 那样 cache 到本地内存或 Tachyon 中。想将 shuffle block cache 到内存中，应该主要是为了提速，但事实上并没有什么必要。</p><p><br/></p><p>首先，内存 cache 发挥作用的前提是被 cache 的数据会被反复使用。使用越频繁，相对来说收益越高。而 shuffle block 只有在下游 task 失败，进行容错恢复时才有重用机会，频次很低。值得注意的是，在不 cache 的情况下，针对同一个含 shuffle 的 RDD 执行多个 action，并不会重用 shuffle 结果。Shuffle block 是按 job ID + task ID + attempt ID 标识的，每个 action 都对应于一个独立的 job，因此无法重用。这里或许是 Spark 的一个可改进点。</p><p><br/></p><p>其次，从数据量上说，如果执行的是需要 shuffle 大数据量的 Spark job，内存容量不够，无论如何都需要落盘；如果执行的是小数据量的 Spark job，虽然 shuffle block 会落盘，但仍然还在 OS cache 内，而 shuffle block 一般都是在生成之后短时间内即被下游 task 取走，所以大部分情况下仍然还是内存访问。</p><p><br/></p><p>最后，将 shuffle block cache 在内存中的确有一个潜在好处，就是有机会直接在内存中保存原始的 Java 对象而避免序列化开销。但这个问题在新近的 Spark 版本中也有了比较好的解决方案。Tungsten project 中引入的 UnsafeRow 格式统一了内存和磁盘表示，已经最小化了序列化成本。</p><p><br/></p><h1>分区函数：HashPartitioner 和 RangePartitioner </h1><div><br/></div><div><span style="font-size: 12pt;">HashPartitioner的原理很简单，只是计算key的hashcode，然后对新分区的数目取余。所以HashPartitioner最重要的属性是新分区的数量。注意HashPartition不能支持key为数组类型。</span></div><div><br/></div><div><span style="font-size: 12pt;">HashPartitioner分区可能导致每个分区中数据量的不均匀。</span></div><div><br/></div><div><span style="font-size: 12pt;">RangePartitioner的原理会稍微复杂一些，会遍历rdd的所有分区数据，从每个分区都会采样，然后根据样本，生成新分区的边界值，这样就可以根据key把数据分布到对应的新分区。</span></div><div><br/></div><div><span style="font-size: 12pt;">Range分区尽量保证每个分区中数据量的均匀，将一定范围内的数映射到某一个分区内。分区与分区之间数据是有序的，但分区内的元素是不能保证顺序的。</span></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 12pt;"><span style="font-weight: bold;">引用</span></span></div><div><a target="_blank" rel="noopener" href="https://zhmin.github.io/2019/01/06/spark-partitioner/" rev="en_rl_none"><span style="font-size: 12pt;">https://zhmin.github.io/2019/01/06/spark-partitioner/</span></a></div><div><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/d9fd44781a32" rev="en_rl_none"><span style="font-size: 12pt;">https://www.jianshu.com/p/d9fd44781a32</span></a></div><div><br/></div><div><br/></div><div><br/></div></body></html></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://xinrihui.github.io">Xinrihui</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xinrihui.github.io/2022/12/25/1.shuffle%20%E7%A0%94%E7%A9%B6/">https://xinrihui.github.io/2022/12/25/1.shuffle%20%E7%A0%94%E7%A9%B6/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/">分布式计算框架</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/25/2.RDD%20%E7%AE%97%E5%AD%90/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Spark RDD 算子</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%20-%E6%95%B0%E6%8D%AEsense/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">计算机组成原理 -数据sense</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/12/25/2.RDD%20%E7%AE%97%E5%AD%90/" title="Spark RDD 算子"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-25</div><div class="title">Spark RDD 算子</div></div></a></div><div><a href="/2022/12/24/MPP%20%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/" title="MPP 数据库原理"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-24</div><div class="title">MPP 数据库原理</div></div></a></div><div><a href="/2022/12/29/hive%20%E8%BF%9E%E6%8E%A5/" title="hive 连接"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-29</div><div class="title">hive 连接</div></div></a></div><div><a href="/2022/12/29/hive%20%E5%8E%BB%E9%87%8D%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%20%E5%92%8C%20group%20by%20%E5%8E%9F%E7%90%86/" title="hive 去重的三种方法 和 group by 原理"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-29</div><div class="title">hive 去重的三种方法 和 group by 原理</div></div></a></div><div><a href="/2022/12/29/hive%20%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/" title="hive 存储格式"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-29</div><div class="title">hive 存储格式</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xinrihui</div><div class="author-info__description">一个计算机民间爱好者的关于算法，分布式系统和机器学习的笔记，欢迎大佬拍砖交流～。笔者曾在微软亚洲研究院实习，参与国家重点研发计划，兴趣为分布式系统和机器学习。转载请注明引用（应该也没人看 ==），邮箱 xinrihui@outlook.com</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">34</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">40</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xinrihui"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">尽量避免使用shuffle类算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.1.</span> <span class="toc-text">方案1.map 端预聚合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.2.</span> <span class="toc-text">方案２.广播</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">数据倾斜问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.1.</span> <span class="toc-text">1.聚合（groupByKey）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.2.</span> <span class="toc-text">２.连接(join)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">3.2.1.</span> <span class="toc-text">1.reduce join 转换为 map join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.采样倾斜的key并分拆join</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">spark SQL 实现 join 的物理计划</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">4.1.</span> <span class="toc-text">1.Broadcast (hash) Join</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">4.2.</span> <span class="toc-text">2.shuffle Hash Join （ 重分区 hash join ）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">4.3.</span> <span class="toc-text">3.shuffle Sort Merge Join （ 重分区 排序 join ）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">有的 join 其实属于 窄依赖 </span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">shuffle管理器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">6.1.</span> <span class="toc-text">HashShuffleManager</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">6.2.</span> <span class="toc-text">SortShuffleManager</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">6.2.1.</span> <span class="toc-text">普通运行机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">6.2.2.</span> <span class="toc-text">bypass 运行机制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">为什么 shuffle 的中间结果落磁盘 而不落在内存</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">分区函数：HashPartitioner 和 RangePartitioner </span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/12/29/Phoenix%20%E5%8E%9F%E7%90%86/" title="Phoenix 原理"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Phoenix 原理"/></a><div class="content"><a class="title" href="/2022/12/29/Phoenix%20%E5%8E%9F%E7%90%86/" title="Phoenix 原理">Phoenix 原理</a><time datetime="2022-12-29T02:26:18.000Z" title="Created 2022-12-29 10:26:18">2022-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/29/hbase%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" title="hbase 性能优化"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hbase 性能优化"/></a><div class="content"><a class="title" href="/2022/12/29/hbase%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" title="hbase 性能优化">hbase 性能优化</a><time datetime="2022-12-29T02:26:18.000Z" title="Created 2022-12-29 10:26:18">2022-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/29/hive%20%E8%BF%9E%E6%8E%A5/" title="hive 连接"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hive 连接"/></a><div class="content"><a class="title" href="/2022/12/29/hive%20%E8%BF%9E%E6%8E%A5/" title="hive 连接">hive 连接</a><time datetime="2022-12-29T02:15:20.000Z" title="Created 2022-12-29 10:15:20">2022-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/29/hive%20%E5%8E%BB%E9%87%8D%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%20%E5%92%8C%20group%20by%20%E5%8E%9F%E7%90%86/" title="hive 去重的三种方法 和 group by 原理"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hive 去重的三种方法 和 group by 原理"/></a><div class="content"><a class="title" href="/2022/12/29/hive%20%E5%8E%BB%E9%87%8D%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%20%E5%92%8C%20group%20by%20%E5%8E%9F%E7%90%86/" title="hive 去重的三种方法 和 group by 原理">hive 去重的三种方法 和 group by 原理</a><time datetime="2022-12-29T02:15:20.000Z" title="Created 2022-12-29 10:15:20">2022-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/29/hive%20%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/" title="hive 存储格式"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hive 存储格式"/></a><div class="content"><a class="title" href="/2022/12/29/hive%20%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/" title="hive 存储格式">hive 存储格式</a><time datetime="2022-12-29T02:15:20.000Z" title="Created 2022-12-29 10:15:20">2022-12-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Xinrihui</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Transformer 论文和源码分析 | 小灰灰在青青草原</title><meta name="author" content="Xinrihui"><meta name="copyright" content="Xinrihui"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer 论文和源码分析Part1. 论文1.Transformer 完全依赖于注意力机制来 表达 输入和输出之间的 全局依赖关系，而没有采用 循环结构 2.Transformer  采用了 编码器-解码器 结构，  （1）编码器将符号表示形式（x1; :::; xn）的输入序列映射到 连续表示形式 z &#x3D;（z1; :::; zn）的序列。  （2）上一步拿到z后，解码器一次生">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 论文和源码分析">
<meta property="og:url" content="https://xinrihui.github.io/2022/12/28/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="小灰灰在青青草原">
<meta property="og:description" content="Transformer 论文和源码分析Part1. 论文1.Transformer 完全依赖于注意力机制来 表达 输入和输出之间的 全局依赖关系，而没有采用 循环结构 2.Transformer  采用了 编码器-解码器 结构，  （1）编码器将符号表示形式（x1; :::; xn）的输入序列映射到 连续表示形式 z &#x3D;（z1; :::; zn）的序列。  （2）上一步拿到z后，解码器一次生">
<meta property="og:locale">
<meta property="article:published_time" content="2022-12-28T06:47:10.000Z">
<meta property="article:modified_time" content="2022-12-28T06:47:10.838Z">
<meta property="article:author" content="Xinrihui">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xinrihui.github.io/2022/12/28/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer 论文和源码分析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-28 14:47:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="小灰灰在青青草原" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">59</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小灰灰在青青草原</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer 论文和源码分析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-12-28T06:47:10.000Z" title="Created 2022-12-28 14:47:10">2022-12-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-12-28T06:47:10.838Z" title="Updated 2022-12-28 14:47:10">2022-12-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/transformer-%E7%B3%BB%E5%88%97/">transformer 系列</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer 论文和源码分析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container">
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 9.6.1 (469462)"/><meta name="author" content="羊村的好朋友小灰灰"/><meta name="created" content="2021-12-03 08:58:45 +0000"/><meta name="source" content="desktop.win"/><meta name="source-application" content="yinxiang.win32"/><meta name="source-url" content="https://notebooks.githubusercontent.com/view/ipynb?azure_maps_enabled=true&amp;browser=chrome&amp;color_mode=auto&amp;commit=53a1be68727b5d5c3a0d0bf18721013843a49041&amp;device=unknown_device&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f74656e736f72666c6f772f74656e736f723274656e736f722f353361316265363837323762356435633361306430626631383732313031333834336134393034312f74656e736f723274656e736f722f6e6f7465626f6f6b732f5472616e73666f726d65725f7472616e736c6174652e6970796e62&amp;logged_in=false&amp;nwo=tensorflow%2Ftensor2tensor&amp;path=tensor2tensor%2Fnotebooks%2FTransformer_translate.ipynb&amp;platform=windows&amp;repository_id=94460704&amp;repository_type=Repository&amp;version=104#14a7c3ce-1400-4b7e-b191-3383004e9937"/><meta name="updated" content="2022-12-28 06:45:47 +0000"/><title>Transformer 论文和源码分析</title></head><body><div><div><div><span style="font-weight: bold;">Part1. 论文</span></div><div><br/></div><div>1.Transformer 完全依赖于注意力机制来 表达 输入和输出之间的 全局依赖关系，而没有采用 循环结构 </div><div><br/></div><div>2.Transformer  采用了 编码器-解码器 结构，</div><div>  （1）编码器将符号表示形式（x1; :::; xn）的输入序列映射到 连续表示形式 z =（z1; :::; zn）的序列。</div><div>  （2）上一步拿到z后，解码器一次生成一个符号的符号的输出序列（y1; :::; ym）。 模型的每一步都是自回归的，在生成下一步的输出时，会将 上一步 生成的符号用作附加输入。</div><div><br/></div><div>3.Multi-Head Attention</div><div><img src="/Resources/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.resources/A3B67D7C-4ED8-4DA2-9072-B6D05E5E71CB.png" height="546" width="447"/><br/></div><div>multi-head attention 是由 h 个 scaled dot-product attention 组成的</div><div><br/></div><div><br/></div><div><span style="font-size: unset; color: unset; font-family: unset;">与 d_model个 维度 一起输入 注意力函数 （ single-head attention ）相比，我们发现 把 总维度 线性投影 h次 有更好的效果。对每一个子空间，我们可以并行的 计算他们的注意力函数，最后得到 dv 维度的 输出value。我们最后会把这些 子空间 连接起来 ，并再次做 线性投影</span></div><div><img src="/Resources/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.resources/4AED5A46-8353-4947-8012-14D393A41632.png" height="266" width="1212"/><br/></div><div>其中 Attention 函数：</div><div><img src="/Resources/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.resources/69411E8B-17AE-4BB0-85B6-C4AE8F7EAEA2.png" height="135" width="1126"/><br/></div><div>总的计算代价，Multi-Head Attention 和 使用全维度的  single-head attention  是一样的</div><div><br/></div><div><br/></div><div>4.我们从三个方面来对比  循环层 、卷积层 和 self-attention层 的处理序列编码-解码的效率：</div><div><br/></div><div>（1）每一层的计算复杂度</div><div><br/></div><div>（2）可以并行化的计算量，以 所需的最少顺序操作数衡量。 对于某个序列 ，self-attention可以直接计算  的点乘结果，而rnn就必须按照顺序从  计算到</div><div><br/></div><div>（3）<span style="font-weight: bold;">网络中远程依赖关系之间的路径长度</span></div><div><br/></div><div>影响学习这种远程依赖关系的一个关键因素是网络中前向和后向信号必须经过的路径长度。</div><div>输入和输出序列中位置的任意组合之间的路径越短，学习远程依赖关系就越容易。</div><div>因此，我们 比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。</div><div><br/></div><div><span style="font-size: medium; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(26, 26, 26); font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, &quot;Source Han Sans SC&quot;, &quot;Noto Sans CJK SC&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">这里Path length指的是要计算一个序列长度为n的信息要经过的路径长度。cnn需要增加卷积层数来扩大视野，rnn需要从1到n逐个进行计算，而self-attention只需要一步矩阵计算就可以。所以也可以看出，self-attention可以比rnn更好地解决长时依赖问题。当然如果计算量太大，比如序列长度n&gt;序列维度d这种情况，也可以用窗口限制self-attention的计算数量</span></div><div><br/></div><ul><li><div><span style="font-weight: bold;">RNN &amp; self-attention</span></div></li></ul><div>self-attention 层使用常数的操作单元（无论句子的长度, t=512）连接所有位置，而 recurrent层则需要 O（n）个 操作单元（取决于 输入序列的长度n） 。</div><div>在计算复杂度方面，当 序列长度n小于表示 维数d时（一般都是 n&lt;d），self-attention层比 recurrent层要快，</div><div>为了提高  长序列任务的计算性能，可以将self-attention 限制为仅考虑输入序列中以各个输出位置为中心的大小为r的邻域，这会将最大路径长度增加到O（n / r）。</div><div><img src="/Resources/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.resources/FAF14F0E-E7B9-4F81-83DC-41C156DE4066.png" height="249" width="1172"/><br/></div><div><br/></div><div>n- 序列的长度  </div><div>d- 中间层的维度</div><div>k- CNN 中卷积核(1D 卷积) 的维度</div><div><br/></div><ul><li><div><span style="font-weight: bold;">CNN &amp;self-attention</span></div></li></ul><div>内核宽度k &lt;n的单个卷积层 无法连接所有成对的输入和输出位置。</div><div>因此，需要 在连续内核的情况下 堆叠 O（n / k）个卷积层，而在膨胀卷积的情况下则需要O（logk（n）），从而增加了任意两个位置之间最长路径的长度 。</div><div>卷积层通常的计算复杂度比循环层 高出k倍。  但是，可分离卷积将复杂度大大降低到O（k·n·d + n·d2）。</div><div>但是，即使k = n，可分离卷积的复杂度也等于self-attention层和 逐点前馈层的组合，而后者是我们所用的结构。</div><div><br/></div><div>5.作为附带的好处，自我关注可以产生更多可解释的模型。各个 attention heads 不仅学会执行不同的任务，而且许多attention heads似乎表现出与句子的句法和语义结构有关的行为。</div><div>即 self-attention模型更可解释，attention结果的分布表明了该模型学习到了一些语法和语义信息</div><div><br/></div><div><span style="font-weight: bold;">6.意义</span></div><div><br/></div><div>transformer 几乎可以用在所有的 NLP 的任务上，后续的工作，像 Bert, GPT 这种很大的预训练模型可以大幅提升所有 NLP 任务的性能，这有点像 CNN 对于计算机视觉的改变：一方面 可以用大量的数据训练一个很大的CNN模型，让其他任务也能从中受益，另一方面为研究者提供了统一的框架， 研究者可以告别之前繁琐的特征提取和建模的工作。原来做NLP 要做文本的预处理，并且根据任务设计模型的架构，我们现在只需要 使用 transformer  就能在所有的任务上取得好的成绩。</div><div><br/></div><div>transformer 不仅仅在NLP 中，在CV 中也取得了很大的进展，同样一个模型在机器学习的所有的领域都能用，一个领域的研究者取得的突破很快可以启发到其他的研究者，另外，人对于自然世界的感知是多模态的，transformer  能把不同的数据融合起来，即把多模态的数据映射到同一个语义空间，使得我们可以做出更大更好的模型。</div><div><br/></div><div><span style="font-size: unset; color: unset; font-family: unset;">Attention 在 模型中的作用是把序列的信息聚合起来，模型中的 MLP 层和 残差连接是不可缺少的。</span><span style="font-size: unset; color: unset; font-family: unset;">Attention 虽然不会对 顺序建模，但是它其实做了一个更广泛的偏纳规制，这么做的代价是 由于假设变得更加一般，模型抓取信息的能力变差了，因此要使用更大的模型和更多的数据才会有好的效果。</span></div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">引用</span></div><div><br/></div><div><span style="font-size: unset; color: unset; font-family: unset;">《Attention Is All You Need》</span></div><div><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/b1030350aadb">https://www.jianshu.com/p/b1030350aadb</a></div><div><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44121378">https://zhuanlan.zhihu.com/p/44121378</a></div></div><div><br/></div><hr/><div><br/></div><div><span style="font-weight: bold;">Part2.</span> <span style="font-weight: bold;">源码分析</span><span style="font-weight: bold;">（tensor2tensor）</span></div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">1.初始化超参数</span></div><div><br/></div><div>PROBLEM = "translate_enfr_wmt32k" # 英语到法语的机器翻译任务  词表大小为32K </div><div>MODEL = "transformer” </div><div><br/></div><div><span style="font-size: unset;"><span style="font-size: unset; color: rgb(255, 0, 0); font-family: unset;">HPARAMS</span></span> <span style="font-size: unset; color: unset; font-family: unset;">= "</span><span style="font-size: unset;"><span style="font-size: unset; color: rgb(255, 0, 0); font-family: unset;">transformer_big</span></span>”<font face="unset"><span style="color: unset; font-size: unset;"> </span></font></div><div><span style="font-size: unset; color: unset; font-family: unset;"># 模型名字，对应论文中的 大</span><span style="font-size: unset; color: unset; font-family: unset;">transformer， 若只有1个GPU 使用 </span><span style="color: rgb(255, 0, 0);">transformer_big_single_gpu</span></div><div># 还可以选择 <span style="color: rgb(255, 0, 0);">transformer_base</span>  对应<span style="font-size: unset; color: unset; font-family: unset;">论文中 标准的 </span><span style="font-size: unset; color: unset; font-family: unset;">transformer</span></div><div>       </div><div>train_steps : 论文提到了在 大的 transformer上使用 8 gpu 时，迭代次数为 300k。因此，如果你有1个gpu，你需要将 迭代次数 x8 或者更多。</div><div>若为 基础的 transformer， train_steps 设置为 100K，100K x 8=800K</div><div><br/></div><div>train_steps <span style="font-weight: bold;">=</span> 300000 # Total number of train steps for all Epochs</div><div>eval_steps <span style="font-weight: bold;">=</span> 100 # Number of steps to perform for each evaluation</div><div>batch_size <span style="font-weight: bold;">=</span> 4096 # 使用动态 batch，表示一个批次的 token 的总数</div><div>save_checkpoints_steps <span style="font-weight: bold;">=</span> 1000</div><div>ALPHA <span style="font-weight: bold;">=</span> 0.1</div><div>schedule <span style="font-weight: bold;">=</span> "continuous_train_and_eval"</div><div><br/></div><div><span style="color: rgb(255, 0, 0);"># 建立模型超参数对象 ，位于 tensor2tensor\utils\hparams_lib.py</span></div><div>hparams = create_hparams(<span style="color: rgb(255, 0, 0);">HPARAMS</span>) </div><div><br/></div><div><span style="color: rgb(255, 0, 0);"># 配置 hparams </span></div><div>hparams.batch_size = batch_size</div><div>hparams.learning_rate<span style="font-size: unset; color: unset; font-family: unset;">= ALPHA</span></div><div><br/></div><div>#hparams.max_length = 256</div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">2.模型对象</span></div><div><br/></div><div><span style="color: rgb(255, 0, 0);"># 位于  tensor2tensor\utils\trainer_lib.py</span></div><div>tensorflow_exp_fn = <span style="color: rgb(255, 0, 0);">create_experiment</span>(</div><div>        run_config=RUN_CONFIG,</div><div>        hparams=hparams,</div><div>        model_name=MODEL,</div><div>        problem_name=PROBLEM,</div><div>        data_dir=DATA_DIR,</div><div>        train_steps=train_steps,</div><div>        eval_steps=eval_steps,</div><div>        #use_xla=True # For acceleration</div><div>    )</div><div><br/></div><div>tensorflow_exp_fn.<span style="color: rgb(255, 0, 0);">train_and_evaluate</span>()</div><div><br/></div><div><span style="font-weight: bold;">2.1 动态 batch </span></div><div><br/></div><div><br/></div><hr/><div><font style="color: rgb(255, 38, 0);">#位于</font><span style="color: rgb(255, 0, 0);"> tensor2tensor\utils\trainer_lib.py</span><br/></div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">create_experiment</span>(</div><div>    run_config,</div><div>    hparams,</div><div>    model_name,</div><div>    problem_name,</div><div>    data_dir,</div><div>    train_steps,</div><div>    eval_steps,</div><div>    .....</div><div><span style="font-size: unset; color: unset; font-family: unset;">):</span></div><div>  </div><div style="margin-left: 40px;"># Input fns from Problem</div><div style="margin-left: 40px;">problem = hparams.problem</div><div style="margin-left: 40px;">train_input_fn = problem.<span style="color: rgb(255, 0, 0);">make_estimator_input_fn</span>(tf_estimator.ModeKeys.TRAIN,</div><div style="margin-left: 40px;">                                                 hparams)</div><div style="margin-left: 40px;"><span style="font-size: unset;"><br/></span></div><div style="margin-left: 40px;"><span style="font-size: unset; color: unset; font-family: unset;">return</span> <span style="font-size: unset; color: rgb(255, 0, 0); font-family: unset;">T2TExperiment</span><span style="font-size: unset; color: unset; font-family: unset;">(estimator, hparams, train_spec, eval_spec,</span></div><div style="margin-left: 40px;">                     use_validation_monitor, decode_hparams)</div><div><br/></div><div><br/></div><div>class <span style="color: rgb(255, 0, 0);">T2TExperiment</span>(object):</div><div>  """Custom Experiment class for running distributed experiments."""</div><div><br/></div><div style="margin-left: 40px;">def <span style="color: rgb(255, 0, 0);">train_and_evaluate</span>(self):</div><div style="margin-left: 40px;">  if self._use_validation_monitor:</div><div style="margin-left: 40px;">    tf.logging.warning("EvalSpec not provided. Estimator will not manage "</div><div style="margin-left: 40px;">                       "model evaluation. Assuming ValidationMonitor present "</div><div style="margin-left: 40px;">                       "in train_hooks.")</div><div style="margin-left: 40px;">    self.<span style="color: rgb(255, 0, 0);">train</span>()</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">def <span style="color: rgb(255, 0, 0);">train</span>(self, max_steps=None):</div><div style="margin-left: 40px;">  mlperf_log.transformer_print(key=mlperf_log.TRAIN_LOOP)</div><div style="margin-left: 40px;">  mlperf_log.transformer_print(key=mlperf_log.TRAIN_EPOCH, value=0)</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  self._estimator.train(</div><div style="margin-left: 40px;">      self._train_spec.<span style="color: rgb(255, 0, 0);">input_fn</span>,</div><div style="margin-left: 40px;">      hooks=self._train_spec.hooks,</div><div style="margin-left: 40px;">      max_steps=max_steps or self._train_spec.max_steps)</div><div><br/></div><div><br/></div><hr/><div><span style="color: rgb(255, 38, 0);">#位于 </span><span style="font-size: unset;"><span style="font-size: unset; color: rgb(255, 0, 0); font-family: unset;">tensor2tensor\data_generators\problem.py</span></span><br/></div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">make_estimator_input_fn</span>(self,</div><div>                            mode,</div><div>                            hparams,</div><div>                            data_dir=None,</div><div>                            force_repeat=False,</div><div>                            prevent_repeat=False,</div><div>                            dataset_kwargs=None):</div><div>  """Return input_fn wrapped for Estimator."""</div><div><br/></div><div style="margin-left: 40px;">  def estimator_input_fn(params, config):</div><div style="margin-left: 40px;">    return self.<span style="color: rgb(255, 0, 0);">input_fn</span>(</div><div style="margin-left: 40px;">        mode,</div><div style="margin-left: 40px;">        hparams,</div><div style="margin-left: 40px;">        data_dir=data_dir,</div><div style="margin-left: 40px;">        params=params,</div><div style="margin-left: 40px;">        config=config,</div><div style="margin-left: 40px;">        force_repeat=force_repeat,</div><div style="margin-left: 40px;">        prevent_repeat=prevent_repeat,</div><div style="margin-left: 40px;">        dataset_kwargs=dataset_kwargs)</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  return estimator_input_fn</div><div><br/></div><div><br/></div><hr/><div><span style="color: rgb(255, 38, 0);">#位于 </span><span style="color: rgb(255, 0, 0);">tensor2tensor\utils\data_reader.py</span><br/></div><div><span style="color: rgb(255, 0, 0);"><br/></span></div><div>def <span style="color: rgb(255, 0, 0);">input_fn</span>(dataset,</div><div>             filepattern,</div><div>             skip_random_fraction_when_training,</div><div>            </div><div><span>    <span>    <span>    </span></span></span> <span style="color: rgb(255, 0, 0);">batch_size_means_tokens_param</span>,  </div><div><span style="color: rgb(255, 0, 0);"><span>    <span>    <span>    </span></span></span># batch size 是否代表 一个批次中 token 的总个数</span></div><div>            </div><div><span>    <span>    <span>    </span></span></span> batch_size_multiplier,</div><div>             max_length,</div><div>             mode,</div><div>             hparams,</div><div>             data_dir=None,</div><div>             params=None,</div><div>             config=None,</div><div>             force_repeat=False,</div><div>             prevent_repeat=False):</div><div><br/></div><div><br/></div><div style="margin-left: 40px;"># Batching</div><div style="margin-left: 40px;">if not <span style="color: rgb(255, 0, 0);">batch_size_means_tokens</span>:</div><div style="margin-left: 40px;">  # Batch size means examples per datashard.</div><div style="margin-left: 40px;">    .....</div><div style="margin-left: 40px;"/><div style="margin-left: 40px;">else:</div><div style="margin-left: 40px;">  # batch_size means tokens per datashard</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 80px;"><span style="color: rgb(255, 0, 0);">cur_batching_scheme</span> = <span style="color: rgb(255, 0, 0);">hparams_to_batching_scheme</span>(</div><div style="margin-left: 80px;">    hparams,</div><div style="margin-left: 80px;">    shard_multiplier=num_shards,</div><div style="margin-left: 80px;">    length_multiplier=batch_size_multiplier)</div><div style="margin-left: 80px;"><br/></div><div style="margin-left: 80px;">dataset = dataset.apply(</div><div style="margin-left: 80px;">    <span style="color: rgb(255, 0, 0);">tf.data.experimental.</span><span style="color: rgb(255, 0, 0);">bucket_by_sequence_length</span>(</div><div style="margin-left: 80px;">        example_length, </div><div style="margin-left: 80px;">        <span style="overflow-x: auto; position: relative; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; border: var(--devsite-inline-nested-code-border,0); border-radius: var(--devsite-inline-code-border-radius,0); word-break: normal; direction: ltr !important; color: var(--devsite-code-color); font-variant-caps: normal; font-variant-ligatures: normal; background-position: 0px center;">bucket_boundaries</span><span style="overflow-x: auto; position: relative; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; border: var(--devsite-inline-nested-code-border,0); border-radius: var(--devsite-inline-code-border-radius,0); word-break: normal; direction: ltr !important; color: var(--devsite-code-color); font-variant-caps: normal; font-variant-ligatures: normal; background-position: 0px center;">=</span><span style="color: rgb(255, 0, 0);">cur_batching_scheme</span>["boundaries"],</div><div style="margin-left: 80px;">        <span style="overflow-x: auto; position: relative; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; border: var(--devsite-inline-nested-code-border,0); border-radius: var(--devsite-inline-code-border-radius,0); word-break: normal; direction: ltr !important; color: var(--devsite-code-color); font-variant-caps: normal; font-variant-ligatures: normal; background-position: 0px center;">bucket_batch_sizes=</span><span style="color: rgb(255, 0, 0);">cur_batching_scheme</span>["batch_sizes"])</div><div style="margin-left: 80px;">)</div><div style="margin-left: 80px;"><span style="color: rgb(255, 0, 0);"># 通过固定 一个批次中 token 的总个数 来动态调整 batch  </span></div><div style="margin-left: 80px;"><span style="color: rgb(255, 0, 0);">#</span><span style="color: rgb(255, 0, 0);"> </span><a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length" style="color: rgb(255, 0, 0);">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length</a></div><div style="margin-left: 40px;"><br/></div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">hparams_to_batching_scheme</span>(hparams,</div><div>                               drop_long_sequences=False,</div><div>                               shard_multiplier=1,</div><div>                               length_multiplier=1):</div><div>  """Wrapper around _batching_scheme with hparams."""</div><div>  </div><div style="margin-left: 40px;">return <span style="color: rgb(255, 0, 0);">batching_scheme</span>(</div><div style="margin-left: 40px;">     <span style="color: rgb(255, 0, 0);"> batch_size=hparams.batch_size, # hparams 中配置的 </span><span style="color: rgb(255, 0, 0);">batch_size</span></div><div style="margin-left: 40px;">      min_length=hparams.min_length,</div><div style="margin-left: 40px;">      max_length=hparams.max_length,</div><div style="margin-left: 40px;">      min_length_bucket=hparams.min_length_bucket,</div><div style="margin-left: 40px;">      length_bucket_step=hparams.length_bucket_step,</div><div style="margin-left: 40px;">      drop_long_sequences=drop_long_sequences,</div><div style="margin-left: 40px;"><span style="font-size: unset; color: unset; font-family: unset;">    )</span></div><div><br/></div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">batching_scheme</span>(batch_size,</div><div>                    max_length,</div><div>                    min_length_bucket,</div><div>                    length_bucket_step,</div><div>                    drop_long_sequences=False,</div><div>                    shard_multiplier=1,</div><div>                    length_multiplier=1,</div><div>                    min_length=0):</div><div>  """A batching scheme based on model hyperparameters.</div><div><br/></div><div>  Args:</div><div><span style="color: rgb(255, 0, 0);">    batch_size: int, total number of tokens in a batch.</span></div><div><br/></div><div>Returns:</div><div>   A dictionary with parameters that can be passed to input_pipeline:</div><div>     * boundaries: list of bucket boundaries <span style="color: rgb(255, 0, 0);"># 不同的 batch 的长度的边界</span></div><div>     * batch_sizes: list of batch sizes for each length bucket <span style="color: rgb(255, 0, 0);"># 每个 batch 的样本的个数</span></div><div><br/></div><div>  """</div><div><br/></div><div><span style="font-weight: bold;">2.2 通过模型名字 注册模型</span></div><div><br/></div><div><br/></div><hr/><div><span style="color: #ff2600;">#位于 </span><span style="color: rgb(255, 0, 0);">tensor2tensor\utils\trainer_lib.py</span></div><div>def <span style="color: rgb(255, 0, 0);">create_experiment</span>(</div><div>    run_config,</div><div>    hparams,</div><div>    model_name,</div><div>    problem_name,</div><div>    data_dir,</div><div>    train_steps,</div><div>    eval_steps,</div><div>    .....</div><div><span style="font-size: unset; color: unset; font-family: unset;">):</span></div><div><br/></div><div style="margin-left: 40px;">estimator = <span style="color: rgb(255, 0, 0);">create_estimator</span>(</div><div style="margin-left: 40px;">    model_name,</div><div style="margin-left: 40px;">    hparams,</div><div style="margin-left: 40px;">    run_config,</div><div style="margin-left: 40px;">    schedule=schedule,</div><div style="margin-left: 40px;">    decode_hparams=decode_hparams,</div><div style="margin-left: 40px;">    use_tpu=use_tpu,</div><div style="margin-left: 40px;">    use_tpu_estimator=use_tpu_estimator,</div><div style="margin-left: 40px;">    use_xla=use_xla,</div><div style="margin-left: 40px;">    export_saved_model_api_version=export_saved_model_api_version,</div><div style="margin-left: 40px;">    use_guarantee_const_getter=use_guarantee_const_getter)</div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">create_estimator</span>(model_name,</div><div>                     hparams,</div><div>                     run_config,</div><div>                     schedule="train_and_evaluate",</div><div><span style="font-size: unset; color: unset; font-family: unset;">):</span></div><div><br/></div><div>  """Create a T2T Estimator."""</div><div style="margin-left: 40px;">  model_fn = t2t_model.T2TModel.<span style="color: rgb(255, 0, 0);">make_estimator_model_fn</span>(<span style="font-size: unset; color: unset; font-family: unset;">model_name, hparams,                                                 decode_hparams=decode_hparams, use_tpu=use_tpu)</span></div><div><br/></div><div><br/></div><hr/><div><br/></div><div><span style="color: #ff2600;">#位于 </span><span style="color: rgb(255, 0, 0);">tensor2tensor\utils\t2t_model.py</span></div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">make_estimator_model_fn</span>(model_name,</div><div>                            hparams,</div><div>                            decode_hparams=None,</div><div>                            use_tpu=False):</div><div><br/></div><div>  model_cls = registry.model(<span style="color: rgb(255, 0, 0);">model_name</span>) <span style="color: rgb(255, 0, 0);"># 使用模型的名字注册模型</span></div><div><br/></div><div><br/></div><div>其中模型的名字定义在 <span style="font-size: unset; color: rgb(255, 0, 0); font-family: unset;">tensor2tensor\tensor2tensor\models\transformer.py</span></div><div><br/></div><div><br/></div><hr/><div><span style="color: #ff2600;">#位于 </span><span style="color: rgb(255, 0, 0);">t</span><span style="font-size: unset; color: rgb(255, 0, 0); font-family: unset;">ensor2tensor\tensor2tensor\models\transformer.py</span></div><div><br/></div><div>@registry.register_hparams</div><div>def <span style="color: rgb(255, 0, 0);">transformer_base_single_gpu</span>():  <span style="color: rgb(255, 0, 0);"># 模型的名字</span></div><div><br/></div><div>  """HParams for transformer base model for single GPU."""</div><div style="margin-left: 40px;">  hparams = <span style="color: rgb(255, 0, 0);">transformer_base</span>()</div><div style="margin-left: 40px;">  hparams.batch_size = 1024</div><div style="margin-left: 40px;">  hparams<span style="color: rgb(255, 0, 0);">.learning_rate_schedule</span> = " <span style="color: rgb(255, 0, 0);">constant </span>* <span style="color: rgb(255, 0, 0);">linear_warmup </span>* <span style="color: rgb(255, 0, 0);">rsqrt_decay </span>" <span style="color: rgb(255, 0, 0);"># 不同学习率策略进行组合</span></div><div style="margin-left: 40px;">  hparams.<span style="color: rgb(255, 0, 0);">learning_rate_constant</span> = 0.1</div><div style="margin-left: 40px;">  hparams.<span style="color: rgb(255, 0, 0);">learning_rate_warmup_steps</span> = 16000</div><div style="margin-left: 40px;">  return hparams</div><div><br/></div><div>@registry.register_hparams</div><div>def <span style="color: rgb(255, 0, 0);">transformer_big_single_gpu</span>():</div><div>  """HParams for transformer big model for single GPU."""</div><div style="margin-left: 40px;">  hparams = <span style="color: rgb(255, 0, 0);">transformer_big</span>()</div><div style="margin-left: 40px;">  hparams.layer_prepostprocess_dropout = 0.1</div><div style="margin-left: 40px;">  hparams.learning_rate_warmup_steps = 16000</div><div style="margin-left: 40px;">  return hparams</div><div><br/></div><div><br/></div><div>@registry.register_hparams</div><div>def <span style="color: rgb(255, 0, 0);">transformer_base_v1</span>():</div><div>  """Set of hyperparameters."""</div><div style="margin-left: 40px;">  hparams = common_hparams.basic_params1()</div><div style="margin-left: 40px;">  hparams.norm_type = "layer"</div><div style="margin-left: 40px;">  hparams.hidden_size = 512</div><div style="margin-left: 40px;">  hparams.batch_size = 4096</div><div style="margin-left: 40px;">  hparams.max_length = 256</div><div style="margin-left: 40px;">  hparams.clip_grad_norm = 0.  # i.e. no gradient clipping</div><div style="margin-left: 40px;">  hparams.optimizer_adam_epsilon = 1e-9</div><div style="margin-left: 40px;"><span style="color: rgb(255, 0, 0);">  hparams.learning_rate_schedule = "legacy" #</span><span style="color: rgb(255, 0, 0);">学习率策略</span></div><div style="margin-left: 40px;"> <span style="color: rgb(255, 0, 0);"> hparams.learning_rate_decay_scheme = "noam"</span></div><div style="margin-left: 40px;">  <span style="color: rgb(255, 0, 0);">hparams.learning_rate = 0.1</span></div><div style="margin-left: 40px;">  hparams.learning_rate_warmup_steps = 4000</div><div style="margin-left: 40px;">  hparams.initializer_gain = 1.0</div><div style="margin-left: 40px;">  hparams.num_hidden_layers = 6</div><div style="margin-left: 40px;">  hparams.initializer = "uniform_unit_scaling"</div><div style="margin-left: 40px;">  hparams.weight_decay = 0.0</div><div style="margin-left: 40px;">  hparams.optimizer_adam_beta1 = 0.9</div><div style="margin-left: 40px;">  hparams.optimizer_adam_beta2 = 0.98</div><div style="margin-left: 40px;">  hparams.num_sampled_classes = 0</div><div style="margin-left: 40px;">  hparams.label_smoothing = 0.1</div><div style="margin-left: 40px;">  hparams.shared_embedding_and_softmax_weights = True</div><div style="margin-left: 40px;">  hparams.symbol_modality_num_shards = 16</div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">2.3 多种学习率策略</span></div><div><br/></div><div><br/></div><hr/><div><br/></div><div><span style="color: rgb(255, 0, 0);"># 位于 tensor2tensor\tensor2tensor\utils\learning_rate.py</span></div><div><br/></div><div>def learning_rate_schedule(hparams):</div><div>  """Learning rate schedule based on hparams."""</div><div><br/></div><div style="margin-left: 40px;">  schedule_string = <span style="color: rgb(255, 0, 0);">hparams.learning_rate_schedule</span></div><div style="margin-left: 40px;">  </div><div style="margin-left: 40px;">  names = schedule_string.split("*") <span style="color: rgb(255, 0, 0);"># 切分出不同的学习率策略</span></div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  names = [name.strip() for name in names if name.strip()]</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  ret = tf.constant(1.0)</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  for name in names:</div><div style="margin-left: 40px;">    ret *= <span style="color: rgb(255, 0, 0);">learning_rate_factor</span>(name, step_num, hparams) <span style="color: rgb(255, 0, 0);"># 不同策略乘起来</span></div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  return ret</div><div><br/></div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">learning_rate_factor</span>(name, step_num, hparams):</div><div>  """Compute the designated learning rate factor from hparams."""</div><div><br/></div><div style="margin-left: 40px;">  if name == "constant":</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">    return hparams.learning_rate_constant</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  elif name == "linear_warmup":</div><div style="margin-left: 40px;">    return <span style="color: rgb(255, 0, 0);">tf.minimum(1.0, step_num / hparams.learning_rate_warmup_steps)</span></div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">elif name == "legacy":</div><div style="margin-left: 40px;">  return <span style="color: rgb(255, 0, 0);">legacy_learning_rate_schedule</span>(hparams)</div><div><br/></div><div><br/></div><div>def <span style="color: rgb(255, 0, 0);">legacy_learning_rate_schedule</span>(hparams):</div><div>  """Backwards-compatible learning-rate schedule."""</div><div><br/></div><div style="margin-left: 40px;">  step_num = _global_step(hparams)</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  warmup_steps = tf.to_float(hparams.learning_rate_warmup_steps)</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  if hparams.learning_rate_decay_scheme == "<span style="color: rgb(255, 0, 0);">noam</span>":</div><div style="margin-left: 40px;">    ret = <span style="color: rgb(255, 0, 0);">5000.0</span> * hparams.hidden_size**-0.5 * tf.minimum(</div><div style="margin-left: 40px;">        (step_num + 1) * warmup_steps**-1.5, (step_num + 1)**-0.5) <span style="color: rgb(255, 0, 0);"># 论文中公式（3）</span></div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  optimizer_correction = <span style="color: rgb(255, 0, 0);">0.002</span> if "adam" in hparams.optimizer else 1.0</div><div style="margin-left: 40px;"><br/></div><div style="margin-left: 40px;">  return ret * optimizer_correction * hparams.<span style="color: rgb(255, 0, 0);">learning_rate </span><span style="color: rgb(255, 0, 0);"># 5000 * 0.002 * 0.1 = 1</span></div><div style="margin-left: 40px;"> </div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">引用</span></div><div><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></div><div><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/Transformer_translate.ipynb">https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/Transformer_translate.ipynb</a></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></div><hr/><hr/><div><br/></div></body></html></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://xinrihui.github.io">Xinrihui</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xinrihui.github.io/2022/12/28/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">https://xinrihui.github.io/2022/12/28/Transformer%20%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/transformer/">transformer</a><a class="post-meta__tags" href="/tags/nlp/">nlp</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/28/4.self-supervised%EF%BC%88Bert%20%EF%BC%89/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Bert的前世今生-3.self-supervised（Bert）</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/25/1.shuffle%20%E7%A0%94%E7%A9%B6/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Spark shuffle 机制</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/12/28/2.%20self-attention/" title="Bert的前世今生-1. self-attention"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-28</div><div class="title">Bert的前世今生-1. self-attention</div></div></a></div><div><a href="/2023/03/10/Attention%20%E6%9C%BA%E5%88%B6%E6%8E%A8%E5%AF%BC/" title="Attention 机制推导"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-10</div><div class="title">Attention 机制推导</div></div></a></div><div><a href="/2022/12/28/3.transformer/" title="Bert的前世今生-2.transformer"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-28</div><div class="title">Bert的前世今生-2.transformer</div></div></a></div><div><a href="/2022/12/28/5.self-supervised%EF%BC%88Bert%20%E5%92%8C%E5%AE%83%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC%EF%BC%89/" title="Bert的前世今生-4.self-supervised（Bert 和它的朋友们）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-28</div><div class="title">Bert的前世今生-4.self-supervised（Bert 和它的朋友们）</div></div></a></div><div><a href="/2022/12/28/4.self-supervised%EF%BC%88Bert%20%EF%BC%89/" title="Bert的前世今生-3.self-supervised（Bert）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-28</div><div class="title">Bert的前世今生-3.self-supervised（Bert）</div></div></a></div><div><a href="/2022/12/04/MLP%20CNN%20RNN%20LSTM%E7%9A%84%E6%8E%A8%E5%AF%BC%EF%BC%88%E6%AD%A3%E5%90%91%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%89/" title="MLP CNN RNN LSTM的推导（正向和反向传播）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-04</div><div class="title">MLP CNN RNN LSTM的推导（正向和反向传播）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xinrihui</div><div class="author-info__description">一个计算机民间爱好者的关于算法，分布式系统和机器学习的笔记，欢迎大佬拍砖交流～。笔者曾在微软亚洲研究院实习，参与国家重点研发计划，兴趣为分布式系统和机器学习。转载请注明引用（应该也没人看 ==），邮箱 xinrihui@outlook.com</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">59</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xinrihui"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/03/16/Evaluation/" title="Evaluation"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Evaluation"/></a><div class="content"><a class="title" href="/2023/03/16/Evaluation/" title="Evaluation">Evaluation</a><time datetime="2023-03-16T08:15:25.000Z" title="Created 2023-03-16 16:15:25">2023-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/16/Learning%20to%20Rank/" title="Learning to Rank"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Learning to Rank"/></a><div class="content"><a class="title" href="/2023/03/16/Learning%20to%20Rank/" title="Learning to Rank">Learning to Rank</a><time datetime="2023-03-16T08:15:25.000Z" title="Created 2023-03-16 16:15:25">2023-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E7%9A%84%20lambda%20%E6%A1%86%E6%9E%B6/" title="大数据系统的 lambda 框架"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大数据系统的 lambda 框架"/></a><div class="content"><a class="title" href="/2023/03/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E7%9A%84%20lambda%20%E6%A1%86%E6%9E%B6/" title="大数据系统的 lambda 框架">大数据系统的 lambda 框架</a><time datetime="2023-03-16T01:22:30.000Z" title="Created 2023-03-16 09:22:30">2023-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/" title="分布式系统综述"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="分布式系统综述"/></a><div class="content"><a class="title" href="/2023/03/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/" title="分布式系统综述">分布式系统综述</a><time datetime="2023-03-10T06:59:05.000Z" title="Created 2023-03-10 14:59:05">2023-03-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/10/Attention%20%E6%9C%BA%E5%88%B6%E6%8E%A8%E5%AF%BC/" title="Attention 机制推导"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Attention 机制推导"/></a><div class="content"><a class="title" href="/2023/03/10/Attention%20%E6%9C%BA%E5%88%B6%E6%8E%A8%E5%AF%BC/" title="Attention 机制推导">Attention 机制推导</a><time datetime="2023-03-10T06:10:51.000Z" title="Created 2023-03-10 14:10:51">2023-03-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Xinrihui</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>